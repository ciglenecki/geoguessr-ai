optimizing:
  config:
    optimizer:
      type: Adam
      kwargs:
        lr: 0.0001
    lr_scheduler: glob(optimizing/config/lr_scheduler/*)
    set_grads_none: true
model:
  type: resnext101_32x8d
  params: {}
  pretrained: true
  unfreeze_layers_num: all
geo:
  crs:
    local_crs: 3766
    country_iso2: hr
  global_crs: 4326
torchvision_version: pytorch/vision:v0.12.0
datamodule:
  image_size: 224
  csv_spacing: 0.7
  dataset_frac: 1
  train_frac: 0.9
  val_frac: 0.05
  test_frac: 0.05
  shuffle_dataset_before_splitting: false
  num_workers: 4
  drop_last: true
  batch_size: 8
  dataset_dirs:
  - /data/original
  - /data/external
trainer:
  log_every_n: 100
  max_epochs: 22
  lr: 2.0e-05
  lr_finetune: 2.0e-05
  finetuning_epoch_period: 5
  early_stopping_epoch_freq: 15
  weight_decay: 0
